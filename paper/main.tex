\documentclass[12pt]{memoir}
\usepackage[backend=bibtex,style=numeric]{biblatex}
\usepackage{simplestyle}
\usepackage{enumitem}

\addbibresource{refs}

\title{Extracting Menu Navigation and Program Configuration Events 
from User Interface Video Tutorials}
\author{Daniel Seita \& Andrew Head}

\begin{document}

\maketitle

\section{Introduction}

\textbf{This work seeks to use video processing and object recognition techniqes as
a technique to mine the usability of an application.}  Given the proliferation
of video tutorials on using different software, this provides new opportunties
to assess what complex actions are frequently performed by a variety of users.
We describe a technique for recognizing menus, detecting that an item has
been clicked in a menu, comparing this menu item to all other instances of
that same item by template matching, and then composing temporally close
menu navigations into configuration tasks.  By working with a corpus of
around 100 tutorials related to setting up the Eclipse environment for
programming tasks, we describe the configuration tasks that are performed
the most often.  We discuss how we can improve this workflow to improve the
speed and accuracy of detection in the future.

\subsection{Motivation}

\textbf{Andrew: Make this have less of a strong focus on the value of
video tutorials, and instead talk about mining usability from online data
sources, such as with CUTS}

Video tutorials have become prevalent for learning software (Matejka et al. 2011, 
Pongnumkul et al. 2010, 1).  Banovic et al. claim this may be because users 
can directly observe expert use, which can serve as a strong learning aid 
(Shneiderman 1983, (1, 4)).  Grossman \& Fitzmaurice showed benefits of short 
contextual video clips (Grossman \& Fitzmaurice 2010, 1).  Short, 10-25 second 
contextually available videos were effective to help users accomplish task and 
retain what they learned than traditional text-based tutorials (Grossman \& 
Fitzmaurice 2010, 4).

Though they also have their drawbacks (Hategekimana et al. 2008, 4):  
Navigation issues could lead to misunderstanding of content (Harrison 1995, 
(1, 4)), and users may not be able to keep up with the pace of the content 
(Palmiter \& Elkerton 1991, 1).  Researchers working with longer, 2-3 minute 
task-oriented tutorials found that users performed better with text tutorials 
than videos because users could not work at the same pace as the video 
(Grabler et al. 2009, 4).  Almost all research on instructional videos point 
to a need for segmented videos that emphasize each step of the task (4).


\section{Related Work}

\textbf{Andrew: This should all use citations, not parentheses.}

\subsection{Reverse Engineering UIs}

Runtime modification of existing interfaces is one application of reverse 
engineeing UIs.  For example, replacing the toolkit drawing object and 
intercepting commands (Edwards et al. 1997, Olsen et al. 1999, 2).  Others 
have used interface introspection using the accessibility API 
(Stuerzlinger et al. 2006, 2).  Others have dynamically loaded code into 
the program space, for example with Scotty (Eagan et al. 2011, 2).

Sikuli (Yeh et al. 2009, 1) uses computer vision to identify GUI elements from 
screen captures, using template matching and voting based on invariant local 
features (Yeh et al. 2009, 2).  Sikuli also searches for UI elements in the 
interface (Bergman et al. 2005, 1), and automates GUI tasks (Chang et al. 2010, 
1).  For searching based on screenshots, 3 types of features are extracted: 
text from the source document, visual words computed from visual words from 
MSER-detected salient elliptical patches, and 3-grams over widget-embedded 
text extracted through OCR (4).  For searching on the screen for saved 
screenshot images, small patterns are found via template matching and large 
patterns through invariant feature voting (4).

Pause-and-Play extends the template matching approach of Sikuli to detect 
tool-selection UI events in compressed, low-resolution video tutorials.  
The system takes a tool palette template and tool icons as input, 
producing metadata file of tool changes with timecodes.  With template 
matching at multiple resolutions, detection is invariant to recording 
resolution and some camera effects.  Their system cannot be run in in 
real-time (3).

A number of different technologies attempt to learn templates for UI 
elements (Chang et al. 2011, Dixon \& Fogarty 2010, Hurst et al. 2010, 
Matejka et al. 2011, 1), using information from mouse APIs (Hurst et al. 
2010, 1) and accessibility APIs (Chang et al. 2011).  Prefab identifies GUI 
elements with GUI specific visual features (Dixon \& Fogarty 2010, 1), which 
enables overlay of advanced interactions on existing interfaces (Dixon et al. 
2011, Dixon et al. 2012, 1).  Waken contributes to this space by discovering 
cursor shapes, clicking actions, icons, tooltips and menus, all by examining 
the difference between frames of UI video.  They do this by making assumptions 
about how the image of icons change during hover and press actions.  Detection 
of the cursor and icons in later frames is performed by template matching (1).

Pixel-based methods for reverse engineering UIs are motivated by their 
indepenedece from underlying application or toolkit requirements (2).  Work 
by Zettleymoyer et al. (Zettlemoyer et al. 1998, Zettlmoyer et al. 1999, 2) 
examine widget identification in IBOTS and VisMap and in programming by 
example (St. Amant et al., Potter, St. Amant et al. 2005, 2).  Olsen et al.'s 
ScreenCrayons links ink annotations to arbitrary screen elements (Olsen et al. 
2004, 2), and Tan et al.'s work interactively subdivides windows with a 
copy-paste metaphor (Tan et al. 2004, 2).

To delve into the technical implementations of the Prefab pixel-based 
matching algorithm, we report from (2).  Prefab identifies interface elements 
with a library of prototypes.  This uses two strategies: exact matches of 
prototype pixels against an image, and modelining the prototype background 
and differencing pixel in an image to identify foreground interface elements.  
Prefab generalizes from example images to models which contain parts like 
exact pixel features, or regions or areas of variable size or repeating 
patterns.  It organizes the entire interface into a hierarchy (2).

\subsection{Action Recognition}

Work here by Jitendra and Georgia and beyond.

\section{Data}

Answer these questions:
\begin{enumerate}[noitemsep]
\item What is our dataset?
\item How did we construct/acquire it?
\item What's its size?
\item (optional) How will we improve it in the future?
\end{enumerate}


\section{Implementation}
Here's a figure of our pipeline.  \textbf{Andrew: Add figure of pipeline.}


\section{Results}

Here we want to touch upon:
\begin{enumerate}[noitemsep]
\item How reliably can we classify UI events?
\item How long does it take for us to process one minute of video?  What are
the practical implications of this?
\item Some images that show recognition results for a few representative
screenshots of user interfaces.
\end{enumerate}


\section{Discussion}

\subsection{Limitations}

\subsection{Future Directions}


\section{Conclusion}
 
We freakin' rock.


\section{References}
\printbibliography[heading=none]

\end{document}
